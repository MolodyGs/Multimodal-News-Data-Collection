{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important**: If you want to use the precomputed local embeddings data (located at /embeddings), please set the variable \"use_embeddings_data\" to True. This will save time by skipping the recalculation of embeddings.\n",
    "\n",
    "\"use_embeddings_data\" equals True will need 4 files:\n",
    "\n",
    "- hybrid_embeddings located at /embeddings\n",
    "- json_data_fact_checking located at /image_model\n",
    "- json_data_fast_check_1_fixed located at /image_model\n",
    "- json_data_fast_check_2_fixed located at /image_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_embeddings_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    # Cargar el modelo CLIP y el procesador\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Función para descargar la imagen desde una URL\n",
    "    def download_image(url):\n",
    "        response = requests.get(url)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_images(data):\n",
    "    pages_checked = data\n",
    "    pages_urls = []\n",
    "    images_downloaded = []\n",
    "    relationships = []\n",
    "    index = 0\n",
    "\n",
    "    for page in data:\n",
    "        images = []\n",
    "        try: \n",
    "            for image in page[\"images\"]:\n",
    "                try:\n",
    "                    download = download_image(image[\"image\"]) \n",
    "                    images_downloaded.append(download)\n",
    "                    images.append(image)\n",
    "                except:\n",
    "                    print(\"No fue posible descargar la imagen. Arreglando\")\n",
    "                    continue\n",
    "            pages_checked[index][\"images\"] = images\n",
    "            relationships.append(len(images))\n",
    "            index += 1\n",
    "            pages_urls += images\n",
    "        except Exception as e:\n",
    "            print(\"error al analizar la pagina\")\n",
    "            print(e)\n",
    "            continue \n",
    "        \n",
    "    print(f\"Imagenes analizadas: {len(pages_urls)}\")\n",
    "    pages_checked = {\"pages\": pages_checked, \"downloaded_images\": images_downloaded, \"relationships\": relationships}\n",
    "    \n",
    "    return pages_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    # Obtiene datos del primero proceso\n",
    "    json_data = {}\n",
    "\n",
    "    with open(\"./json_data_fast_check_1_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    json_data_fast_check_1_fixed = check_images(json_data[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    # Obtiene datos del segundo proceso\n",
    "    json_data = {}\n",
    "\n",
    "    with open(\"./json_data_fast_check_2_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    json_data_fast_check_2_fixed = check_images(json_data[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    # Obtiene datos de fact checking\n",
    "    json_data = {}\n",
    "\n",
    "    with open(\"./json_data_fact_checking.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    json_data_fact_checking = check_images(json_data[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=json_data_fast_check_1_fixed[\"downloaded_images\"], return_tensors=\"pt\")\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "        image_embeddings = outputs\n",
    "        torch.save(image_embeddings, '../embeddings/1st_image_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=json_data_fast_check_2_fixed[\"downloaded_images\"], return_tensors=\"pt\")\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "        image_embeddings = outputs\n",
    "        torch.save(image_embeddings, '../embeddings/2nd_image_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=json_data_fact_checking[\"downloaded_images\"], return_tensors=\"pt\")\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "        image_embeddings = outputs\n",
    "        torch.save(image_embeddings, '../embeddings/fact_checking_image_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    json_data_fact_checking = {\"pages\": json_data_fact_checking[\"pages\"]}\n",
    "    json_data_fast_check_1_fixed = {\"pages\": json_data_fast_check_1_fixed[\"pages\"]}\n",
    "    json_data_fast_check_2_fixed = {\"pages\": json_data_fast_check_2_fixed[\"pages\"]}\n",
    "\n",
    "    with open(f\"json_data_fact_checking.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data_fact_checking, file, ensure_ascii=False, indent=4)\n",
    "    with open(f\"json_data_fast_check_1_fixed.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data_fast_check_1_fixed, file, ensure_ascii=False, indent=4)\n",
    "    with open(f\"json_data_fast_check_2_fixed.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data_fast_check_2_fixed, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "else: \n",
    "    json_data_fact_checking = {}\n",
    "    json_data_fast_check_1_fixed = {}\n",
    "    json_data_fast_check_2_fixed = {}\n",
    "\n",
    "    with open(f\"json_data_fact_checking.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data_fact_checking = json.load(file)\n",
    "    with open(f\"json_data_fast_check_1_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data_fast_check_1_fixed = json.load(file)\n",
    "    with open(f\"json_data_fast_check_2_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data_fast_check_2_fixed = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estructura de datos para guardar las relaciones entre las imagenes y las noticias\n",
    "relationships = []\n",
    "image_relationships = []\n",
    "for page in json_data_fast_check_1_fixed[\"pages\"]:\n",
    "    relationships.append(len(page[\"images\"]))\n",
    "    for image in page[\"images\"]:\n",
    "        image_relationships.append({\"image\": image, \"page\": page})\n",
    "\n",
    "for page in json_data_fast_check_2_fixed[\"pages\"]:\n",
    "    relationships.append(len(page[\"images\"]))\n",
    "    for image in page[\"images\"]:\n",
    "        image_relationships.append({\"image\": image, \"page\": page})\n",
    "\n",
    "for page in json_data_fact_checking[\"pages\"]:\n",
    "    relationships.append(len(page[\"images\"]))\n",
    "    for image in page[\"images\"]:\n",
    "        image_relationships.append({\"image\": image, \"page\": page})\n",
    "\n",
    "#Crea una lista de cluster para luego visualizar que imagen pertenece a que noticia.\n",
    "amount = 0\n",
    "cluster_relationships = []\n",
    "for index, relation in enumerate(relationships):\n",
    "    amount += relation\n",
    "    if relation == 0:\n",
    "        continue\n",
    "    for i in range(relation):\n",
    "        cluster_relationships.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    image_embeddings_1st = torch.load('../embeddings/1st_image_embeddings.pt')\n",
    "    image_embeddings_2nd = torch.load('../embeddings/2nd_image_embeddings.pt')\n",
    "    text_embeddings_1st = torch.load('../embeddings/1st_text_embeddings.pt')\n",
    "    text_embeddings_2nd = torch.load('../embeddings/2nd_text_embeddings.pt')\n",
    "    image_embeddings_fact_checking = torch.load('../embeddings/fact_checking_image_embeddings.pt')\n",
    "    text_embeddings_fact_checking = torch.load('../embeddings/fact_checking_text_embeddings.pt')\n",
    "\n",
    "    #Texto\n",
    "    text_embeddings_1st = torch.from_numpy(text_embeddings_1st).float() \n",
    "    text_embeddings_2nd = torch.from_numpy(text_embeddings_2nd).float() \n",
    "    text_embeddings_fact_checking = torch.from_numpy(text_embeddings_fact_checking).float() \n",
    "    hybrid_text_embeddings = torch.cat((text_embeddings_1st, text_embeddings_2nd, text_embeddings_fact_checking), dim=0)\n",
    "    hybrid_text_embeddings = torch.cat(\n",
    "        [hybrid_text_embeddings[i].unsqueeze(0).repeat(n, 1) for i, n in enumerate(relationships) if n != 0],\n",
    "        dim=0\n",
    "    )\n",
    "\n",
    "    #Images\n",
    "    hybrid_image_embeddings = torch.cat((image_embeddings_1st, image_embeddings_2nd, image_embeddings_fact_checking), dim=0)\n",
    "\n",
    "    #Concatenación\n",
    "    print(hybrid_text_embeddings.shape)\n",
    "    print(hybrid_image_embeddings.shape)\n",
    "    hybrid_embeddings = torch.cat((hybrid_text_embeddings, hybrid_image_embeddings), dim=1)\n",
    "\n",
    "    #Forma Final\n",
    "    print(hybrid_embeddings.shape)\n",
    "    torch.save(hybrid_embeddings, '../embeddings/hybrid_embeddings.pt')\n",
    "else: \n",
    "    hybrid_embeddings = torch.load('../embeddings/hybrid_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Important**\n",
    "\n",
    "# If you want to use the precomputed local embeddings data (located at /embeddings), please set the variable \"use_embeddings_data\" to **True** at the start of this code. This will save time by avoiding the recalculation of embeddings.\n",
    "\n",
    "# If you want to recalculate the embeddings, set the variable \"use_embedding_data\" to **False** at the start of this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model= UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_model.fit_transform(hybrid_embeddings)\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1, cluster_selection_epsilon=0.01)\n",
    "cluster_labels = clusterer.fit_predict(embeddings_2d)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', s=10)\n",
    "plt.title('Results using text and image embeddings')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "distances = pairwise_distances(embeddings_2d, metric='euclidean')\n",
    "average_distance = np.mean(distances)\n",
    "min_distance = np.min(distances[distances > 0])\n",
    "max_distance = np.max(distances)\n",
    "\n",
    "print(f\"Distancia promedio: {average_distance}\")\n",
    "print(f\"Distancia mínima: {min_distance}\")\n",
    "print(f\"Distancia máxima: {max_distance}\")\n",
    "\n",
    "score = silhouette_score(embeddings_2d, cluster_labels)\n",
    "print(f\"Silhouette Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_cluster_to_print = 22\n",
    "facts_selected = []\n",
    "facts_labels = []\n",
    "\n",
    "for index, fact in enumerate(embeddings_2d):\n",
    "    if cluster_labels[index] == hdbscan_cluster_to_print:\n",
    "        facts_selected.append(embeddings_2d[index].tolist())\n",
    "        facts_labels.append(index)\n",
    "\n",
    "plt.scatter([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], s=10)\n",
    "for x, y, label in zip([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], facts_labels):\n",
    "    plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=(0, 3), ha='center')\n",
    "plt.title(f'Focusing on Cluster {hdbscan_cluster_to_print}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "json_data_combined = json_data_fast_check_1_fixed[\"pages\"] + json_data_fast_check_2_fixed[\"pages\"]\n",
    "for index in facts_labels:\n",
    "    print(f\"[{index}]   Imagen: {image_relationships[index]['image']}\")\n",
    "    print(f\"[{index}]   Pagina: {image_relationships[index]['page']['link']}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_relationships, cmap='viridis', s=10)\n",
    "plt.title('Images Associated with Each Text')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_print = 175\n",
    "\n",
    "json_data_combined = json_data_fast_check_1_fixed[\"pages\"] + json_data_fast_check_2_fixed[\"pages\"]\n",
    "\n",
    "facts_selected = []\n",
    "facts_labels = []\n",
    "\n",
    "for index, fact in enumerate(embeddings_2d):\n",
    "    if cluster_relationships[index] == cluster_to_print:\n",
    "        facts_selected.append(embeddings_2d[index].tolist())\n",
    "        facts_labels.append(index)\n",
    "\n",
    "labels = []\n",
    "labels_index = []\n",
    "labels_relation = []\n",
    "\n",
    "for index, image in enumerate(image_relationships):\n",
    "    if index in facts_labels:\n",
    "        labels_relation.append(image[\"page\"][\"veracity\"])\n",
    "        if image[\"page\"][\"veracity\"] not in labels:\n",
    "            labels.append(image[\"page\"][\"veracity\"])\n",
    "        labels_index.append(labels.index(image[\"page\"][\"veracity\"]))\n",
    "\n",
    "plt.scatter([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], s=10)\n",
    "\n",
    "for x, y, etiqueta in zip([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], facts_labels):\n",
    "    plt.annotate(etiqueta, (x, y), textcoords=\"offset points\", xytext=(0, 3), ha='center')\n",
    "\n",
    "plt.title(f'Focusing on Cluster {cluster_to_print} | index of the images')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "for index in facts_labels:\n",
    "    print(f\"[{index}]   Imagen: {image_relationships[index]['image']}\")\n",
    "    print(f\"[{index}]   Pagina: {image_relationships[index]['page']['link']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "labels_index = []\n",
    "labels_relation = []\n",
    "colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'black', 'pink', 'brown', 'gray', 'cyan']\n",
    "\n",
    "for image in image_relationships:\n",
    "    labels_relation.append(image[\"page\"][\"veracity\"])\n",
    "    if image[\"page\"][\"veracity\"] not in labels:\n",
    "        labels.append(image[\"page\"][\"veracity\"])\n",
    "    labels_index.append(labels.index(image[\"page\"][\"veracity\"]))\n",
    "\n",
    "print(labels)\n",
    "print(colors)\n",
    "\n",
    "for i in range(len(embeddings_2d)):\n",
    "    color = colors[labels.index(labels_relation[i])]\n",
    "    plt.plot(embeddings_2d[:, 0][i], embeddings_2d[:, 1][i], marker='o', linestyle='', label=labels_relation[i], ms=4, color=color) \n",
    "\n",
    "plt.title('Veracity by Colors')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
