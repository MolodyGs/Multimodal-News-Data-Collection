{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Representation of Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, XLMRobertaTokenizer, XLMRobertaModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.patches as mpatches\n",
    "import embedding_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1st_process = {}\n",
    "text_2nd_process = {}\n",
    "fact_checking_text = {}\n",
    "biobiochile_text = {}\n",
    "row_data = {\"pages\": []}\n",
    "json_data = {}\n",
    "\n",
    "with open(\"../../data/json_data_fast_check_1_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)  \n",
    "    row_data[\"pages\"] += json_data[\"pages\"]\n",
    "text_1st_process = [page[\"text\"] for page in json_data[\"pages\"]]\n",
    "\n",
    "with open(\"../../data/json_data_fast_check_2_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)  \n",
    "    row_data[\"pages\"] += json_data[\"pages\"]\n",
    "text_2nd_process = [page[\"text\"] for page in json_data[\"pages\"]]\n",
    "\n",
    "with open(\"../../data/json_data_fact_checking_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)  \n",
    "    row_data[\"pages\"] += json_data[\"pages\"]\n",
    "fact_checking_text = [page[\"text\"] for page in json_data[\"pages\"]]\n",
    "\n",
    "with open(\"../../data/json_data_biobiochile_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)  \n",
    "    row_data[\"pages\"] += json_data[\"pages\"]\n",
    "biobiochile_text = [page[\"text\"] for page in json_data[\"pages\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling\n",
    "\n",
    "## 2.1 Using BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, truncation=False)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    encodings = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1st = get_embeddings(text_1st_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2st = get_embeddings(text_2nd_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fact_checking = get_embeddings(fact_checking_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_biobiochile = get_embeddings(biobiochile_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings_1st, '1st_BERT_text_embeddings.pt')\n",
    "torch.save(embeddings_2st, '2nd_BERT_text_embeddings.pt')\n",
    "torch.save(embeddings_fact_checking, 'fact_checking_BERT_text_embeddings.pt')\n",
    "torch.save(embeddings_biobiochile, 'biobiochile_BERT_text_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Using RoBERTa (larger than BERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_RoBERTa(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    sentence_embeddings = last_hidden_states.mean(dim=1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1st = get_embeddings_RoBERTa(text_1st_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2st = get_embeddings_RoBERTa(text_2nd_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fact_checking = get_embeddings_RoBERTa(fact_checking_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_biobiochile = get_embeddings_RoBERTa(biobiochile_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings_1st, '1st_RoBERTa_text_embeddings.pt')\n",
    "torch.save(embeddings_2st, '2nd_RoBERTa_text_embeddings.pt')\n",
    "torch.save(embeddings_fact_checking, 'fact_checking_RoBERTa_text_embeddings.pt')\n",
    "torch.save(embeddings_biobiochile, 'biobioChile_RoBERTa_text_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1st_file_name = '1st_RoBERTa_text_embeddings.pt'\n",
    "embeddings_2nd_file_name = '2nd_RoBERTa_text_embeddings.pt'\n",
    "embeddings_fact_checking_file_name = 'fact_checking_RoBERTa_text_embeddings.pt'\n",
    "embeddings_biobiochile_file_name = 'biobiochile_RoBERTa_text_embeddings.pt'\n",
    "\n",
    "# embeddings_1st_file_name = '1st_BERT_text_embeddings.pt'\n",
    "# embeddings_2nd_file_name = '2nd_BERT_text_embeddings.pt'\n",
    "# embeddings_fact_checking_file_name = 'fact_checking_BERT_text_embeddings.pt'\n",
    "# embeddings_biobiochile_file_name = 'biobiochile_BERT_text_embeddings.pt'\n",
    "\n",
    "embeddings_1st = torch.load(embeddings_1st_file_name)\n",
    "embeddings_2st = torch.load(embeddings_2nd_file_name)\n",
    "embeddings_fact_checking = torch.load(embeddings_fact_checking_file_name)\n",
    "embeddings_biobiochile = torch.load(embeddings_biobiochile_file_name)\n",
    "embeddings = np.concatenate([embeddings_1st, embeddings_2st, embeddings_fact_checking, embeddings_biobiochile])\n",
    "\n",
    "print(embeddings_1st.shape)\n",
    "print(embeddings_2st.shape)\n",
    "print(embeddings_fact_checking.shape)\n",
    "print(embeddings_biobiochile.shape)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = []\n",
    "image_relationships = []\n",
    "\n",
    "for page in row_data[\"pages\"]:\n",
    "    relationships.append(len(page[\"images\"]))\n",
    "    for image in page[\"images\"]:\n",
    "        image_relationships.append({\"image\": image, \"page\": page})\n",
    "\n",
    "amount = 0\n",
    "cluster_relationships = []\n",
    "for index, relation in enumerate(relationships):\n",
    "    amount += relation\n",
    "    if relation == 0:\n",
    "        continue\n",
    "    for i in range(relation):\n",
    "        cluster_relationships.append(index)\n",
    "\n",
    "print(cluster_relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. t-SNE 2D Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=1, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "cluster_labels = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, cluster_selection_epsilon=0.6).fit_predict(embeddings_2d)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', s=10)\n",
    "plt.title('Results using t-SNE and HDBSCAN')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "distances = pairwise_distances(embeddings_2d, metric='euclidean')\n",
    "average_distance = np.mean(distances)\n",
    "min_distance = np.min(distances[distances > 0])\n",
    "max_distance = np.max(distances)\n",
    "\n",
    "print(f\"Distancia promedio: {average_distance}\")\n",
    "print(f\"Distancia mínima: {min_distance}\")\n",
    "print(f\"Distancia máxima: {max_distance}\")\n",
    "\n",
    "score = silhouette_score(embeddings_2d, cluster_labels)\n",
    "print(f\"Silhouette Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT\n",
    "\n",
    "- Distancia promedio: 88.40070343017578\n",
    "- Distancia mínima: 0.47465968132019043\n",
    "- Distancia máxima: 276.6690979003906\n",
    "- Silhouette Score: 0.6445894241333008\n",
    "\n",
    "RoBERTa\n",
    "\n",
    "- Distancia promedio: 83.52395629882812\n",
    "- Distancia mínima: 0.3482244312763214\n",
    "- Distancia máxima: 264.5120849609375\n",
    "- Silhouette Score: 0.6999763250350952\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'fact_checking_data': 'red', 'fast_check_1st': 'blue', 'fast_check_2nd': 'green'}\n",
    "\n",
    "cluster_to_print = -1\n",
    "facts_selected = []\n",
    "facts_labels = []\n",
    "\n",
    "for index, fact in enumerate(embeddings_2d):\n",
    "    if cluster_labels[index] == cluster_to_print:\n",
    "        facts_selected.append(embeddings_2d[index].tolist())\n",
    "        facts_labels.append(index)\n",
    "\n",
    "plt.scatter([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], s=10)\n",
    "for x, y, etiqueta in zip([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], facts_labels):\n",
    "    plt.annotate(etiqueta, (x, y), textcoords=\"offset points\", xytext=(0, 3), ha='center')\n",
    "\n",
    "legend_labels = [mpatches.Patch(color=color, label=label) for label, color in colors.items()]\n",
    "plt.legend(handles=legend_labels, title=\"Sites\", fontsize=8)\n",
    "plt.title(f'Focusing on Cluster {cluster_to_print}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. UMAP 2D Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betters values for HDBSCAN and UMAP\n",
    "# cluster_selection_epsilon_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 2.5, 3]\n",
    "# min_cluster_size_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# min_samples_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# rondom_states = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "# silhouette_scores = []\n",
    "# for random_state in rondom_states:\n",
    "#     umap_model= UMAP(n_components=2, random_state=random_state)\n",
    "#     embeddings_2d = umap_model.fit_transform(hybrid_embeddings)\n",
    "#     for epsilon in cluster_selection_epsilon_values:\n",
    "#         for min_cluster_size in min_cluster_size_values:\n",
    "#             for min_samples in min_samples_values:\n",
    "#                 clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, cluster_selection_epsilon=epsilon)\n",
    "#                 cluster_labels = clusterer.fit_predict(embeddings_2d)\n",
    "#                 score = silhouette_score(embeddings_2d, cluster_labels)\n",
    "#                 silhouette_scores.append({\"score\": score, \"epsilon\": epsilon, \"min_cluster_size\": min_cluster_size, \"min_samples\": min_samples, \"random_state\": random_state})\n",
    "# silhouette_scores = pd.DataFrame(silhouette_scores)\n",
    "# silhouette_scores = silhouette_scores.sort_values(by='score', ascending=False)\n",
    "# print(silhouette_scores.head(1)) # Best silhouette score: 0.734589, epsilon: 2.5 , min_cluster_size: 4 , min_samples: 5, random_state: 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_components=2, random_state=12, n_neighbors=3, min_dist=0.0, n_jobs=1, init='random')\n",
    "embeddings_2d = umap_model.fit_transform(embeddings)\n",
    "cluster_labels = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3, cluster_selection_epsilon=2.2).fit_predict(embeddings_2d)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, s=10)\n",
    "plt.title('Results using UMAP and HDBSCAN')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "distances = pairwise_distances(embeddings_2d, metric='euclidean')\n",
    "average_distance = np.mean(distances)\n",
    "min_distance = np.min(distances[distances > 0])  \n",
    "max_distance = np.max(distances)\n",
    "print(f\"Distancia promedio: {average_distance}\")\n",
    "print(f\"Distancia mínima: {min_distance}\")\n",
    "print(f\"Distancia máxima: {max_distance}\")\n",
    "\n",
    "score = silhouette_score(embeddings_2d, cluster_labels)\n",
    "print(f\"Silhouette Score: {score}\")\n",
    "\n",
    "pages_embeddings_info =[]\n",
    "for index, emb in enumerate(embeddings_2d):\n",
    "    pages_embeddings_info.append(embedding_data.emb_data(emb, index, row_data[\"pages\"][index], cluster_labels[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT\n",
    "\n",
    "- Distancia promedio: 7.3546671867370605\n",
    "- Distancia mínima: 0.0007768553914502263\n",
    "- Distancia máxima: 25.34515953063965\n",
    "- Silhouette Score: 0.6593766808509827\n",
    "\n",
    "RoBERTa\n",
    "\n",
    "- Distancia promedio: 5.941552639007568\n",
    "- Distancia mínima: 2.4616068913019262e-05\n",
    "- Distancia máxima: 23.637327194213867\n",
    "- Silhouette Score: 0.672860324382782\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_print = 10\n",
    "embeddings_selected = []\n",
    "facts_labels = []\n",
    "for index, emb in enumerate(pages_embeddings_info):\n",
    "    if emb.get_cluster() == cluster_to_print:\n",
    "        embeddings_selected.append(emb)\n",
    "\n",
    "for emb, etiqueta in zip([emb.get_embedding() for emb in embeddings_selected], [emb.get_label() for emb in embeddings_selected]):\n",
    "    x = emb[0]\n",
    "    y = emb[1]\n",
    "    plt.annotate(etiqueta, (x, y), textcoords=\"offset points\", xytext=(0, 6), ha='center', color=\"blue\")\n",
    "    plt.scatter(x, y, s=10, color=\"blue\")\n",
    "\n",
    "plt.title(f'RoBERTa | Focusing on Cluster {cluster_to_print}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "for emb in embeddings_selected:\n",
    "    print(f\"{emb.get_label()} - {emb.get_page()['link']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show spceific news\n",
    "fact_index = [18, 61, 97, 122, 130, 132, 135, 137, 144, 146, 153, 158, 166]\n",
    "\n",
    "facts_labels = []\n",
    "color_index = []\n",
    "for index in range(len(embeddings_2d)):\n",
    "    facts_labels.append(index)\n",
    "    if index in fact_index:\n",
    "        color_index.append(2)\n",
    "        continue\n",
    "    color_index.append(0)\n",
    "    \n",
    "colors = ['#e2a6a6', '#3e63ef', '#36fa32']\n",
    "plot_index = 0\n",
    "for x, y, etiqueta in zip([fact[0] for fact in embeddings_2d], [fact[1] for fact in embeddings_2d], facts_labels):\n",
    "    plt.scatter(x, y, s=10, c=colors[color_index[plot_index]], zorder=color_index[plot_index])\n",
    "    plot_index += 1\n",
    "\n",
    "plt.title(f'RoBERTa | Focusing on Specific News')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "for index, page in enumerate(row_data[\"pages\"]):\n",
    "    if index in facts_labels:\n",
    "        print(f\"[{index}] link: {page['link']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_inndex = 18\n",
    "cluster_to_print = 0\n",
    "for emb in pages_embeddings_info:\n",
    "    if emb.get_label() == page_inndex:\n",
    "        cluster_to_print = emb.get_cluster()\n",
    "        break\n",
    "\n",
    "embeddings_to_show = []\n",
    "pages_labels = []\n",
    "for emb in pages_embeddings_info:\n",
    "    if emb.get_cluster() == cluster_to_print:\n",
    "        embeddings_to_show.append(emb)\n",
    "        pages_labels.append(emb.get_label())\n",
    "\n",
    "plt.scatter([emb.get_embedding()[0] for emb in embeddings_to_show], [emb.get_embedding()[1] for emb in embeddings_to_show], s=10)\n",
    "\n",
    "reference_data = [emb.get_embedding()[0] for emb in embeddings_to_show]\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "for x, y, etiqueta in zip([emb.get_embedding()[0] for emb in embeddings_to_show], [emb.get_embedding()[1] for emb in embeddings_to_show], pages_labels):\n",
    "    reference = 0\n",
    "    times = 0\n",
    "    for index, cordx in enumerate(reference_data):\n",
    "        if cordx == x:\n",
    "            continue\n",
    "        diff = abs(cordx - x)\n",
    "        if diff < 0.1:\n",
    "            reference += 4\n",
    "            times += 1\n",
    "            del reference_data[index]\n",
    "    if times >= 3:\n",
    "        times = int(times/3)\n",
    "        if times >= 3:\n",
    "            times = 2\n",
    "\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "    if etiqueta == 132:\n",
    "        x_offset = 15\n",
    "        y_offset = -9\n",
    "\n",
    "    if etiqueta == 152:\n",
    "        x_offset = -13\n",
    "        y_offset = -12\n",
    "\n",
    "    if etiqueta == 146:\n",
    "        x_offset = -15\n",
    "        y_offset = -5\n",
    "\n",
    "    if etiqueta == 61:\n",
    "        x_offset = 0\n",
    "        y_offset = -8\n",
    "\n",
    "    if etiqueta == 166:\n",
    "        x_offset = -15\n",
    "        y_offset = -18\n",
    "\n",
    "    if etiqueta == 144:\n",
    "        x_offset = 0\n",
    "        y_offset = -30\n",
    "\n",
    "    if etiqueta == 158:\n",
    "        x_offset = 10\n",
    "        y_offset = 0\n",
    "\n",
    "    if etiqueta == 135:\n",
    "        x_offset = -10\n",
    "        y_offset = 0\n",
    "\n",
    "    if etiqueta == 97:\n",
    "        x_offset = -5\n",
    "        y_offset = -5\n",
    "\n",
    "    if etiqueta == 130:\n",
    "        x_offset = -10\n",
    "        y_offset = 0\n",
    "\n",
    "    if etiqueta == 263:\n",
    "        x_offset = 10\n",
    "        y_offset = -2\n",
    "    plt.annotate(etiqueta, (x, y), textcoords=\"offset points\", xytext=(0 + x_offset, 6 + reference + y_offset), ha='center', color=colors[times])\n",
    "    plt.scatter(x, y, s=10, c=colors[times])\n",
    "\n",
    "plt.title(f'Results using UMAP and HDBSCAN | Cluster: {cluster_to_print}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.xlim(4.9, 6)\n",
    "plt.ylim(-4.2, -3.0)\n",
    "plt.show()\n",
    "\n",
    "for emb in embeddings_to_show:\n",
    "    print(f\"{emb.get_label()} - {emb.get_page()['link']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
