{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "from umap import UMAP\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_embeddings_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_fact_checking = {}\n",
    "json_data_fast_check_1_fixed = {}\n",
    "json_data_fast_check_2_fixed = {}\n",
    "\n",
    "with open(f\"../image_model/json_data_fact_checking.json\", 'r', encoding='utf-8') as file:\n",
    "    json_data_fact_checking = json.load(file)\n",
    "with open(f\"../image_model/json_data_fast_check_1_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "    json_data_fast_check_1_fixed = json.load(file)\n",
    "with open(f\"../image_model/json_data_fast_check_2_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "    json_data_fast_check_2_fixed = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(pages):\n",
    "    image_embeds = []\n",
    "    text_embeds = []\n",
    "\n",
    "    for page in pages:\n",
    "        if len(page[\"images\"]) == 0:\n",
    "            continue\n",
    "\n",
    "        images = []\n",
    "        for image in page[\"images\"]:\n",
    "            response = requests.get(image[\"image\"])\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            images.append(image)\n",
    "\n",
    "        batch_size = 8  # Ajusta según la memoria\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch_images = images[i:i+batch_size]\n",
    "            batch_texts = [page[\"text\"][:77]] * len(batch_images)  # Limita longitud del texto\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = processor(images=batch_images, text=batch_texts, return_tensors=\"pt\", padding=True)\n",
    "                outputs = model(**inputs)\n",
    "                image_embeds.append(outputs.image_embeds)\n",
    "                text_embeds.append(outputs.text_embeds)\n",
    "\n",
    "    image_embeds = torch.cat(image_embeds, dim=0)\n",
    "    text_embeds = torch.cat(text_embeds, dim=0)\n",
    "    embeddings = torch.cat((image_embeds, text_embeds), dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fast_check_1 = extract_embeddings(json_data_fast_check_1_fixed[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fast_check_2 = extract_embeddings(json_data_fast_check_2_fixed[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fact_checking = extract_embeddings(json_data_fact_checking[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_embeddings = torch.cat((embeddings_fast_check_1, embeddings_fast_check_2, embeddings_fact_checking), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings_fast_check_1, 'multimodal_embeddings_fast_check_1.pt')\n",
    "torch.save(embeddings_fast_check_2, 'multimodal_embeddings_fast_check_2.pt')\n",
    "torch.save(embeddings_fact_checking, 'multimodal_embeddings_fact_checking.pt')\n",
    "torch.save(multimodal_embeddings, 'multimodal_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use saved embeddings\n",
    "multimodal_embeddings = torch.load('multimodal_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = []\n",
    "image_relationships = []\n",
    "for page in json_data_fast_check_1_fixed[\"pages\"]:\n",
    "    relationships.append(len(page[\"images\"]))\n",
    "    for image in page[\"images\"]:\n",
    "        image_relationships.append({\"image\": image, \"page\": page})\n",
    "\n",
    "for page in json_data_fast_check_2_fixed[\"pages\"]:\n",
    "    relationships.append(len(page[\"images\"]))\n",
    "    for image in page[\"images\"]:\n",
    "        image_relationships.append({\"image\": image, \"page\": page})\n",
    "\n",
    "for page in json_data_fact_checking[\"pages\"]:\n",
    "    relationships.append(len(page[\"images\"]))\n",
    "    for image in page[\"images\"]:\n",
    "        image_relationships.append({\"image\": image, \"page\": page})\n",
    "\n",
    "amount = 0\n",
    "cluster_relationships = []\n",
    "for index, relation in enumerate(relationships):\n",
    "    amount += relation\n",
    "    if relation == 0:\n",
    "        continue\n",
    "    for i in range(relation):\n",
    "        cluster_relationships.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model= UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_model.fit_transform(multimodal_embeddings.detach().numpy())\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1, cluster_selection_epsilon=0.41)\n",
    "cluster_labels = clusterer.fit_predict(embeddings_2d)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', s=10)\n",
    "plt.title('Results using text and image embeddings')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "distances = pairwise_distances(embeddings_2d, metric='euclidean')\n",
    "average_distance = np.mean(distances)\n",
    "min_distance = np.min(distances[distances > 0])\n",
    "max_distance = np.max(distances)\n",
    "\n",
    "print(f\"Distancia promedio: {average_distance}\")\n",
    "print(f\"Distancia mínima: {min_distance}\")\n",
    "print(f\"Distancia máxima: {max_distance}\")\n",
    "\n",
    "score = silhouette_score(embeddings_2d, cluster_labels)\n",
    "print(f\"Silhouette Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_cluster_to_print = 0\n",
    "facts_selected = []\n",
    "facts_labels = []\n",
    "\n",
    "for index, fact in enumerate(embeddings_2d):\n",
    "    if cluster_labels[index] == hdbscan_cluster_to_print:\n",
    "        facts_selected.append(embeddings_2d[index].tolist())\n",
    "        facts_labels.append(index)\n",
    "\n",
    "plt.scatter([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], s=10)\n",
    "for x, y, label in zip([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], facts_labels):\n",
    "    plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=(0, 3), ha='center')\n",
    "plt.title(f'Focusing on Cluster {hdbscan_cluster_to_print}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "for index in facts_labels:\n",
    "    print(f\"[{index}]   Imagen: {image_relationships[index]['image']['image']}\")\n",
    "    print(f\"[{index}]   Pagina: {image_relationships[index]['page']['link']}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
