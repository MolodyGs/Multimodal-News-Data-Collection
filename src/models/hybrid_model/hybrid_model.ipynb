{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as kimage\n",
    "import io\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker\n",
    "import embedding_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important**: If you want to use the precomputed local embeddings data (located at /embeddings), please set the variable \"use_embeddings_data\" to True. This will save time by skipping the recalculation of embeddings.\n",
    "\n",
    "\"use_embeddings_data\" equals True will need 4 files:\n",
    "\n",
    "- hybrid_embeddings located at /embeddings\n",
    "- json_data_fact_checking located at /image_model\n",
    "- json_data_fast_check_1_fixed located at /image_model\n",
    "- json_data_fast_check_2_fixed located at /image_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_embeddings_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    model = EfficientNetB0(weights=\"imagenet\", include_top=False, pooling=\"avg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def check_images(data):\n",
    "    pages_checked = data\n",
    "    pages_urls = []\n",
    "    images_downloaded = []\n",
    "    relationships = []\n",
    "    index = 0\n",
    "\n",
    "    for page in data:\n",
    "        images = []\n",
    "        try: \n",
    "            for image in page[\"images\"]:\n",
    "                try:\n",
    "                    download = download_image(image[\"image\"]) \n",
    "                    images_downloaded.append(download)\n",
    "                    images.append(image)\n",
    "                except:\n",
    "                    print(f\"[{image['image']}]No fue posible descargar la imagen. Arreglando\")\n",
    "                    continue\n",
    "            pages_checked[index][\"images\"] = images\n",
    "            relationships.append(len(images))\n",
    "            index += 1\n",
    "            pages_urls += images\n",
    "        except Exception as e:\n",
    "            print(\"error al analizar la pagina\")\n",
    "            print(e)\n",
    "            continue \n",
    "        \n",
    "    print(f\"Imagenes analizadas: {len(pages_urls)}\")\n",
    "    pages_checked = {\"pages\": pages_checked, \"downloaded_images\": images_downloaded, \"relationships\": relationships}\n",
    "    \n",
    "    return pages_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    json_data = {}\n",
    "    with open(\"../../data/json_data_fast_check_1_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "    json_data_fast_check_1_fixed = check_images(json_data[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    json_data = {}\n",
    "    with open(\"../../data/json_data_fast_check_2_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "    json_data_fast_check_2_fixed = check_images(json_data[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    json_data = {}\n",
    "    with open(\"../../data/json_data_fact_checking_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "    json_data_fact_checking_fixed = check_images(json_data[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    json_data = {}\n",
    "    with open(\"../../data/json_data_biobiochile_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "    json_data_biobiochile_fixed = check_images(json_data[\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(images):\n",
    "    with torch.no_grad():\n",
    "        processed_images = []\n",
    "        for pil_image in images[\"downloaded_images\"]:\n",
    "            image_bytes = io.BytesIO()\n",
    "            pil_image.save(image_bytes, format=\"JPEG\")\n",
    "            image_bytes.seek(0)\n",
    "            img = kimage.load_img(image_bytes, target_size=(224, 224))\n",
    "            img_array = kimage.img_to_array(img)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            processed_images.append(img_array)\n",
    "        batch = np.array(processed_images)\n",
    "        image_embeddings = model.predict(batch)\n",
    "        return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    image_embeddings_fast_check_1_fixed = get_image_embeddings(json_data_fast_check_1_fixed)\n",
    "    image_embeddings_fast_check_2_fixed = get_image_embeddings(json_data_fast_check_2_fixed)\n",
    "    image_embeddings_biobiochile_fixed = get_image_embeddings(json_data_biobiochile_fixed)\n",
    "    image_embeddings_fact_checking_fixed = get_image_embeddings(json_data_fact_checking_fixed)\n",
    "\n",
    "    torch.save(image_embeddings_fast_check_1_fixed, '1st_image_embeddings.pt')\n",
    "    torch.save(image_embeddings_fast_check_2_fixed, '2nd_image_embeddings.pt')\n",
    "    torch.save(image_embeddings_biobiochile_fixed, 'biobiochile_image_embeddings.pt')\n",
    "    torch.save(image_embeddings_fact_checking_fixed, 'fact_checking_image_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_embeddings_data: \n",
    "    json_data_fact_checking = {\"pages\": json_data_fact_checking_fixed[\"pages\"]}\n",
    "    json_data_fast_check_1 = {\"pages\": json_data_fast_check_1_fixed[\"pages\"]}\n",
    "    json_data_fast_check_2 = {\"pages\": json_data_fast_check_2_fixed[\"pages\"]}\n",
    "    json_data_biobiochile = {\"pages\": json_data_biobiochile_fixed[\"pages\"]}\n",
    "    row_data = {\"pages\": []}\n",
    "    row_data = json_data_fact_checking_fixed[\"pages\"] + json_data_fast_check_1_fixed[\"pages\"] + json_data_fast_check_2_fixed[\"pages\"] + json_data_biobiochile_fixed[\"pages\"]\n",
    "\n",
    "    with open(f\"../../data/json_data_fact_checking_fixed.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data_fact_checking, file, ensure_ascii=False, indent=4)\n",
    "    with open(f\"../../data/json_data_fast_check_1_fixed.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data_fast_check_1, file, ensure_ascii=False, indent=4)\n",
    "    with open(f\"../../data/json_data_fast_check_2_fixed.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data_fast_check_2, file, ensure_ascii=False, indent=4)\n",
    "    with open(f\"../../data/json_data_biobiochile_fixed.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data_biobiochile, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "else: \n",
    "    json_data_fact_checking = {}\n",
    "    json_data_fast_check_1 = {}\n",
    "    json_data_fast_check_2 = {}\n",
    "    json_data_biobiochile = {}\n",
    "    row_data = {\"pages\": []}\n",
    "\n",
    "    with open(f\"../../data/json_data_fact_checking_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data_fact_checking = json.load(file)\n",
    "        row_data[\"pages\"] = json_data_fact_checking[\"pages\"]\n",
    "    with open(f\"../../data/json_data_fast_check_1_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data_fast_check_1 = json.load(file)\n",
    "        row_data[\"pages\"] += json_data_fast_check_1[\"pages\"]\n",
    "    with open(f\"../../data/json_data_fast_check_2_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data_fast_check_2 = json.load(file)\n",
    "        row_data[\"pages\"] += json_data_fast_check_2[\"pages\"]\n",
    "    with open(f\"../../data/json_data_biobiochile_fixed.json\", 'r', encoding='utf-8') as file:\n",
    "        json_data_biobiochile = json.load(file)\n",
    "        row_data[\"pages\"] += json_data_biobiochile[\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = []\n",
    "image_relationships = []\n",
    "\n",
    "all_pages = json_data_fast_check_1[\"pages\"] + json_data_fast_check_2[\"pages\"] + json_data_fact_checking[\"pages\"] + json_data_biobiochile[\"pages\"]\n",
    "    \n",
    "for page in all_pages:\n",
    "    relationships.append(len(page[\"images\"]))\n",
    "    for image in page[\"images\"]:\n",
    "        image_relationships.append({\"image\": image, \"page\": page})\n",
    "\n",
    "amount = 0\n",
    "cluster_relationships = []\n",
    "for index, relation in enumerate(relationships):\n",
    "    amount += relation\n",
    "    if relation == 0:\n",
    "        continue\n",
    "    for i in range(relation):\n",
    "        cluster_relationships.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images = 0\n",
    "\n",
    "for page in all_pages:\n",
    "    count_images += len(page[\"images\"])\n",
    "\n",
    "print(f\"Total de imagenes: {count_images}\")\n",
    "\n",
    "print(len(cluster_relationships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings_1st = torch.load('1st_image_embeddings.pt')\n",
    "image_embeddings_2nd = torch.load('2nd_image_embeddings.pt')\n",
    "image_embeddings_fact_checking = torch.load('fact_checking_image_embeddings.pt')\n",
    "image_embeddings_biobiochile = torch.load('biobiochile_image_embeddings.pt')\n",
    "text_embeddings_1st = torch.load('../text_model/1st_RoBERTa_text_embeddings.pt')\n",
    "text_embeddings_2nd = torch.load('../text_model/2nd_RoBERTa_text_embeddings.pt')\n",
    "text_embeddings_fact_checking = torch.load('../text_model/fact_checking_RoBERTa_text_embeddings.pt')\n",
    "text_embeddings_biobiochile = torch.load('../text_model/biobiochile_RoBERTa_text_embeddings.pt')\n",
    "\n",
    "#Text\n",
    "if not isinstance(text_embeddings_biobiochile, torch.Tensor):\n",
    "    text_embeddings_biobiochile = torch.tensor(text_embeddings_biobiochile.tolist())\n",
    "if not isinstance(text_embeddings_fact_checking, torch.Tensor):\n",
    "    text_embeddings_fact_checking = torch.tensor(text_embeddings_fact_checking.tolist())\n",
    "if not isinstance(text_embeddings_1st, torch.Tensor):\n",
    "    text_embeddings_1st = torch.tensor(text_embeddings_1st.tolist())\n",
    "if not isinstance(text_embeddings_2nd, torch.Tensor):\n",
    "    text_embeddings_2nd = torch.tensor(text_embeddings_2nd.tolist())\n",
    "\n",
    "hybrid_text_embeddings = torch.cat((text_embeddings_1st, text_embeddings_2nd, text_embeddings_fact_checking, text_embeddings_biobiochile), dim=0)\n",
    "hybrid_text_embeddings = torch.cat(\n",
    "    [hybrid_text_embeddings[i].unsqueeze(0).repeat(n, 1) for i, n in enumerate(relationships) if n != 0],\n",
    "    dim=0\n",
    ")\n",
    "\n",
    "#Images\n",
    "if not isinstance(image_embeddings_1st, torch.Tensor):\n",
    "    image_embeddings_1st = torch.tensor(image_embeddings_1st.tolist())\n",
    "if not isinstance(image_embeddings_2nd, torch.Tensor):\n",
    "    image_embeddings_2nd = torch.tensor(image_embeddings_2nd.tolist())\n",
    "if not isinstance(image_embeddings_fact_checking, torch.Tensor):\n",
    "    image_embeddings_fact_checking = torch.tensor(image_embeddings_fact_checking.tolist())\n",
    "if not isinstance(image_embeddings_biobiochile, torch.Tensor):\n",
    "    image_embeddings_biobiochile = torch.tensor(image_embeddings_biobiochile.tolist())\n",
    "\n",
    "hybrid_image_embeddings = torch.cat((image_embeddings_1st, image_embeddings_2nd, image_embeddings_fact_checking, image_embeddings_biobiochile), dim=0)\n",
    "\n",
    "#Concatenación\n",
    "print(f\"text shape: {hybrid_text_embeddings.shape}\")\n",
    "print(f\"image shape: {hybrid_image_embeddings.shape}\")\n",
    "hybrid_embeddings = torch.cat((hybrid_text_embeddings, hybrid_image_embeddings), dim=1)\n",
    "\n",
    "#Forma Final\n",
    "print(hybrid_embeddings.shape)\n",
    "torch.save(hybrid_embeddings, 'hybrid_embeddings.pt')\n",
    "hybrid_embeddings = torch.load('hybrid_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Important**\n",
    "\n",
    "# If you want to use the precomputed local embeddings data (located at /embeddings), please set the variable \"use_embeddings_data\" to **True** at the start of this code. This will save time by avoiding the recalculation of embeddings.\n",
    "\n",
    "# If you want to recalculate the embeddings, set the variable \"use_embedding_data\" to **False** at the start of this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model= UMAP(n_components=2, random_state=70)\n",
    "embeddings_2d = umap_model.fit_transform(hybrid_embeddings)\n",
    "\n",
    "# cluster_selection_epsilon_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 2.5, 3]\n",
    "# min_cluster_size_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# min_samples_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# rondom_states = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "# silhouette_scores = []\n",
    "# for random_state in rondom_states:\n",
    "#     umap_model= UMAP(n_components=2, random_state=random_state)\n",
    "#     embeddings_2d = umap_model.fit_transform(hybrid_embeddings)\n",
    "#     for epsilon in cluster_selection_epsilon_values:\n",
    "#         for min_cluster_size in min_cluster_size_values:\n",
    "#             for min_samples in min_samples_values:\n",
    "#                 clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, cluster_selection_epsilon=epsilon)\n",
    "#                 cluster_labels = clusterer.fit_predict(embeddings_2d)\n",
    "#                 score = silhouette_score(embeddings_2d, cluster_labels)\n",
    "#                 silhouette_scores.append({\"score\": score, \"epsilon\": epsilon, \"min_cluster_size\": min_cluster_size, \"min_samples\": min_samples, \"random_state\": random_state})\n",
    "# silhouette_scores = pd.DataFrame(silhouette_scores)\n",
    "# silhouette_scores = silhouette_scores.sort_values(by='score', ascending=False)\n",
    "# print(silhouette_scores.head(1)) # Best silhouette score: 0.734589, epsilon: 2.5 , min_cluster_size: 4 , min_samples: 5, random_state: 70\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=4, min_samples=5, cluster_selection_epsilon=2.5)\n",
    "cluster_labels = clusterer.fit_predict(embeddings_2d)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', s=10)\n",
    "plt.title('Results using text and image embeddings')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "colorbar = plt.colorbar(label='Cluster Label')\n",
    "colorbar.set_ticks(range(int(min(cluster_labels)), int(max(cluster_labels)) + 1))\n",
    "colorbar.ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: int(x)))\n",
    "plt.show()\n",
    "\n",
    "distances = pairwise_distances(embeddings_2d, metric='euclidean')\n",
    "average_distance = np.mean(distances)\n",
    "min_distance = np.min(distances[distances > 0])\n",
    "max_distance = np.max(distances)\n",
    "\n",
    "print(f\"Distancia promedio: {average_distance}\")\n",
    "print(f\"Distancia mínima: {min_distance}\")\n",
    "print(f\"Distancia máxima: {max_distance}\")\n",
    "\n",
    "score = silhouette_score(embeddings_2d, cluster_labels)\n",
    "print(f\"Silhouette Score: {score}\")\n",
    "\n",
    "pages_embeddings_info =[]\n",
    "for index, emb in enumerate(embeddings_2d):\n",
    "    pages_embeddings_info.append(embedding_data.emb_data(emb, index, row_data[\"pages\"][index], cluster_labels[index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_print = 0\n",
    "embeddings_selected = []\n",
    "facts_labels = []\n",
    "for index, emb in enumerate(pages_embeddings_info):\n",
    "    if emb.get_cluster() == cluster_to_print:\n",
    "        embeddings_selected.append(emb)\n",
    "\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for emb, etiqueta in zip([emb.get_embedding() for emb in embeddings_selected], [emb.get_label() for emb in embeddings_selected]):\n",
    "    x = emb[0]\n",
    "    y = emb[1]\n",
    "    plt.annotate(etiqueta, (x, y), textcoords=\"offset points\", xytext=(0, 6), ha='center', color=\"blue\")\n",
    "    plt.scatter(x, y, s=10, color=\"blue\")\n",
    "\n",
    "plt.title(f'Cluster {cluster_to_print} | Hybrid Embeddings')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "for emb in embeddings_selected:\n",
    "    print(f\"{emb.get_label()} - {emb.get_page()['link']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_relationships, cmap='viridis', s=10)\n",
    "plt.title('Images Associated with Each Text')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_to_print = 303\n",
    "\n",
    "facts_selected = []\n",
    "facts_labels = []\n",
    "\n",
    "for index, fact in enumerate(embeddings_2d):\n",
    "    if cluster_relationships[index] == page_to_print:\n",
    "        facts_selected.append(embeddings_2d[index].tolist())\n",
    "        facts_labels.append(index)\n",
    "\n",
    "labels = []\n",
    "labels_index = []\n",
    "labels_relation = []\n",
    "\n",
    "for index, image in enumerate(image_relationships):\n",
    "    if index in facts_labels:\n",
    "        labels_relation.append(image[\"page\"][\"veracity\"])\n",
    "        if image[\"page\"][\"veracity\"] not in labels:\n",
    "            labels.append(image[\"page\"][\"veracity\"])\n",
    "        labels_index.append(labels.index(image[\"page\"][\"veracity\"]))\n",
    "\n",
    "plt.scatter([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], s=10)\n",
    "\n",
    "for x, y, etiqueta in zip([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], facts_labels):\n",
    "    plt.annotate(etiqueta, (x, y), textcoords=\"offset points\", xytext=(0, 3), ha='center')\n",
    "\n",
    "plt.title(f'Focusing on page {page_to_print} | index of the images')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "for index in facts_labels:\n",
    "    print(f\"[{index}]   Imagen: {image_relationships[index]['image']['image']}\")\n",
    "    print(f\"[{index}]   Pagina: {image_relationships[index]['page']['link']}\")\n",
    "    print(f\"[{index}]   indice de página: {cluster_relationships[index]}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Cluster by image index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 416\n",
    "hdbscan_cluster_to_print = cluster_labels[image_index]\n",
    "print(hdbscan_cluster_to_print)\n",
    "facts_selected = []\n",
    "facts_labels = []\n",
    "text_cluster = []\n",
    "\n",
    "for index, fact in enumerate(embeddings_2d):\n",
    "    if cluster_labels[index] == hdbscan_cluster_to_print:\n",
    "        facts_selected.append(embeddings_2d[index].tolist())\n",
    "        facts_labels.append(index)\n",
    "        text_cluster.append(cluster_relationships[index])\n",
    "\n",
    "plt.scatter([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], c=text_cluster, cmap='viridis', s=10)\n",
    "for x, y, label in zip([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], facts_labels):\n",
    "    plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=(0, 3), ha='center')\n",
    "plt.title(f'Focusing on Cluster {hdbscan_cluster_to_print}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "for index in facts_labels:\n",
    "    print(f\"[{index}]   Imagen: {image_relationships[index]['image']['image']}\")\n",
    "    print(f\"[{index}]   Pagina: {image_relationships[index]['page']['link']}\")\n",
    "    print(f\"[{index}]   indice de página: {cluster_relationships[index]}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show cluster by fact index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una misma noticia, como tiene varias imagenes, es posible entonces que la noticia como tal esté en más de un cluster.\n",
    "fact_index = 0\n",
    "\n",
    "if fact_index not in cluster_relationships:\n",
    "    print(\"No se encontró la noticia en los clusters\")\n",
    "    exit()\n",
    "\n",
    "hdbscan_clusters_to_print = []\n",
    "for index, relation in enumerate(cluster_relationships):\n",
    "    if relation == fact_index:\n",
    "        hdbscan_clusters_to_print.append(cluster_labels[index])\n",
    "hdbscan_clusters_to_print = list(set(hdbscan_clusters_to_print))\n",
    "\n",
    "if len(hdbscan_clusters_to_print) == 0:\n",
    "    print(\"No se encontraron clusters asociados a la noticia\")\n",
    "    exit()\n",
    "\n",
    "print(\"clusters de HDBSCAN asociados a la noticia: \")\n",
    "for cluster in hdbscan_clusters_to_print:\n",
    "    print(f\"[{cluster}]\")\n",
    "\n",
    "\n",
    "facts_selected = []\n",
    "facts_labels = []\n",
    "text_cluster = []\n",
    "for index, fact in enumerate(embeddings_2d):\n",
    "    if cluster_labels[index] == hdbscan_clusters_to_print[0]:\n",
    "        facts_selected.append(embeddings_2d[index].tolist())\n",
    "        facts_labels.append(index)\n",
    "        text_cluster.append(cluster_relationships[index])\n",
    "\n",
    "\n",
    "plt.scatter([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], c=text_cluster, cmap='viridis', s=10)\n",
    "for x, y, label in zip([fact[0] for fact in facts_selected], [fact[1] for fact in facts_selected], facts_labels):\n",
    "    plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=(0, 3), ha='center')\n",
    "plt.title(f'Focusing on Cluster {hdbscan_clusters_to_print[0]}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for index in facts_labels:\n",
    "    print(f\"[{index}]   Imagen: {image_relationships[index]['image']['image']}\")\n",
    "    print(f\"[{index}]   Pagina: {image_relationships[index]['page']['link']}\")\n",
    "    print(f\"[{index}]   indice de página: {cluster_relationships[index]}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
