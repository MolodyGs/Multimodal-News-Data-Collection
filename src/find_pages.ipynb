{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets pages on Google News published between November 15, 2019 and December 17, 2023\n",
    "\n",
    "The data is extracted by looking for the following websites:\n",
    "\n",
    "- Fast Check\n",
    "- Fact Checking UC\n",
    "- Biobio Chile\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pages from Fast Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(start, end, term, document_path):\n",
    "\n",
    "    search_term = term\n",
    "    search_term = search_term.replace(' ', '+')  \n",
    "    num_pages = 20  \n",
    "    start_date = start\n",
    "    start_year = int(start_date.split(\"/\")[2])\n",
    "    end_date = end\n",
    "    end_year = int(end_date.split(\"/\")[2])\n",
    "    json_pages_info = {\n",
    "        \"pages\": []\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Chrome/58.0.3029.110'\n",
    "    }\n",
    "\n",
    "    skipped_pages = 0\n",
    "\n",
    "\n",
    "    all_pages_links = []\n",
    "\n",
    "    for page in range(0, num_pages):\n",
    "        url = f'https://www.google.com/search?q={search_term}&tbm=nws&start={page*10}&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}'\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"status 200\")\n",
    "\n",
    "            response.encoding = 'ISO-8859-1'\n",
    "            content = response.content.decode('ISO-8859-1')\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            with open('output.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(soup.prettify())\n",
    "\n",
    "            pages_section = soup.find_all('div', class_='Gx5Zad xpd EtOod pkphOe')\n",
    "\n",
    "            for page_section in pages_section:\n",
    "                link_section = page_section.find('a', href=True)\n",
    "                href = link_section['href']\n",
    "\n",
    "                if(href.startswith(\"/url?q=\")):\n",
    "                    \n",
    "                    link = href.split(\"/url?q=\")[1].split(\"&\")[0]\n",
    "                    if (all_pages_links.count(link) > 0):\n",
    "                        print(all_pages_links)\n",
    "                        print(\"ya ingresada\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        all_pages_links.append(link)\n",
    "\n",
    "                    date = page_section.find('span', class_='r0bn4c rQMQod')\n",
    "                    date = date.text.split(\" \")\n",
    "                    if(date[2] != \"meses\" and date[2] != \"años\"):\n",
    "                        continue\n",
    "                    date = datetime.now() - relativedelta(months=int(date[1]))\n",
    "                    if(date.year < start_year or date.year > end_year):\n",
    "                        skipped_pages += 1\n",
    "                        continue\n",
    "                    date = date.strftime(\"%B\") + \", \" + str(date.year)\n",
    "\n",
    "                    newscast = page_section.find('div', class_='BNeawe UPmit AP7Wnd lRVwie')\n",
    "                    title = page_section.find('div', class_='BNeawe vvjwJb AP7Wnd')\n",
    "                    description = page_section.find('div', class_='BNeawe s3v9rd AP7Wnd')\n",
    "                    image = page_section.find('img', class_=\"h1hFNe\")\n",
    "                    \n",
    "                    if image is None:\n",
    "                        image = \"none\"\n",
    "                    else: \n",
    "                        image = image['src']\n",
    "                    \n",
    "                    link_info = {\n",
    "                        \"newscast\" : newscast.text,\n",
    "                        \"title\": title.text,\n",
    "                        \"description\": description.text,\n",
    "                        \"link\": link,\n",
    "                        \"date\": f\"(preliminary) {date}\",\n",
    "                        \"image\": image,\n",
    "                        \"author\": \"not yet extracted\",\n",
    "                        \"text\": \"not yet extracted\",\n",
    "                        \"links\": \"not yet extracted\"\n",
    "                    }\n",
    "                    json_pages_info[\"pages\"].append(link_info)\n",
    "        else:\n",
    "            print(f\"Error al acceder a la página {page + 1}: {response.status_code}\")\n",
    "\n",
    "    print(f\"Se han encontrado: {len(json_pages_info['pages'])} páginas\")\n",
    "\n",
    "    with open(f\"{document_path}\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_pages_info, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast Check | Primero proceso constitucional\n",
    "get_pages('11/15/2019', '05/14/2022', 'Constitución Chile Fast Check', './archive/fast_check_data/1st_fast_check_pages.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast Check | Segundo proceso constitucional\n",
    "get_pages('05/14/2022', '12/17/2023', 'Constitución Chile Fast Check', './archive/fast_check_data/2nd_fast_check_pages.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pages('11/15/2019', '12/17/2023', 'Constitución Chile biobioChile', './archive/biobiochile_data/biobiochile_pages.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Others Pages besides Fact Check and Fast Checking UC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pages('11/15/2019', '05/14/2022', 'Constitución Chile', 'others_pages.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extract from Biobio Chile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\n",
    "    \"enero\",\n",
    "    \"febrero\",\n",
    "    \"marzo\",\n",
    "    \"abril\",\n",
    "    \"mayo\",\n",
    "    \"junio\",\n",
    "    \"julio\",\n",
    "    \"agosto\",\n",
    "    \"septiembre\",\n",
    "    \"octubre\",\n",
    "    \"noviembre\",\n",
    "    \"diciembre\"\n",
    "]\n",
    "\n",
    "biobiochile_variations = [\n",
    "    \"especial/nuevo-proceso-constituyente\",\n",
    "    \"especial/una-constitucion-para-chile\",\n",
    "    \"noticias/nacional\"\n",
    "]\n",
    "\n",
    "def biobiochile_data_extract(): \n",
    "    biobiochile_links = {}\n",
    "    biobiochile_data_extracted = {\"pages\" : []}\n",
    "\n",
    "    with open(f\"./archive/biobiochile_data/biobiochile_pages.json\", 'r', encoding='utf-8') as file:\n",
    "        biobiochile_links = json.load(file)\n",
    "\n",
    "    for page in biobiochile_links[\"pages\"]:\n",
    "        link = page[\"link\"]\n",
    "        try:\n",
    "            link_split = link.split(\"/\")\n",
    "            variation = link_split[3] + \"/\" + link_split[4]\n",
    "            variation_index = biobiochile_variations.index(variation)\n",
    "            page_data = {}\n",
    "            if variation_index == -1:\n",
    "                print(Fore.RED + f\"Error: {link} no es un link de noticias o especial\" + Style.RESET_ALL)\n",
    "            if variation_index == 0:\n",
    "                page_data = biobiochile_special_new_process(page)\n",
    "            if variation_index == 1:\n",
    "                page_data = biobiochile_special_a_constitution(page)\n",
    "            if variation_index == 2 or link_split[3] == \"noticias\":\n",
    "                page_data = biobiochile_news(page)\n",
    "            biobiochile_data_extracted[\"pages\"].append(page_data)\n",
    "        except:\n",
    "            print(Fore.RED + f\"Error: {link} no se pudo procesar\" + Style.RESET_ALL)\n",
    "    \n",
    "    with open(f\"./archive/biobiochile_data/biobiochile_data_extracted.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(biobiochile_data_extracted, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def biobiochile_news(page):\n",
    "    link = page[\"link\"]\n",
    "    soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "    date = soup.find(\"div\", class_=\"post-date\")\n",
    "    if date is None:\n",
    "        date = \"no year found\"\n",
    "    else:\n",
    "        date = date.text.split(\" \")\n",
    "        month = date[2]\n",
    "        if month in months:\n",
    "            month = months.index(month) + 1\n",
    "            date = f\"{date[1]}/{month}/{date[4]}\"\n",
    "    title = soup.find(\"h1\", class_=\"post-title\").text\n",
    "    author = soup.find(\"div\", class_=\"autores\").text\n",
    "    text = \"\".join([p.text for p in soup.find(\"div\", class_=\"post-content clearfix\").find_all(\"p\")])\n",
    "    links = list(set([a[\"href\"] for a in soup.find(\"div\", class_=\"post-content clearfix\").find_all(\"a\")]))\n",
    "    images = [{\"image\": soup.find(\"div\", class_=\"post-image\").find(\"a\")[\"href\"], \"text\": \"\", \"error\": \"\"}]\n",
    "    page_data = {\n",
    "        \"link\": link,\n",
    "        \"veracity\": \"Es un sitio que no verifica noticias\",\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"author\": author,\n",
    "        \"text\": text,\n",
    "        \"links\": links,\n",
    "        \"images\": images\n",
    "    }\n",
    "    return page_data\n",
    "    \n",
    "def biobiochile_special_a_constitution(page):\n",
    "    link = page[\"link\"]\n",
    "    soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "    date = soup.find(\"div\", class_=\"fecha\")\n",
    "    if date is None:\n",
    "        date = \"no year found\"\n",
    "    else:\n",
    "        date = date.text.split(\" \")\n",
    "        month = date[2]\n",
    "        if month in months:\n",
    "            month = months.index(month) + 1\n",
    "            date = f\"{date[1]}/{month}/{date[4]}\"\n",
    "    title = soup.find(\"h1\", class_=\"titular\").text\n",
    "    author = soup.find(\"div\", class_=\"autores\").text.split(\"Por\")[0]\n",
    "    text = soup.find(\"div\", class_=\"contenido-nota\").text\n",
    "    links = \"\"\n",
    "    images = []\n",
    "    page_data = {\n",
    "        \"link\": link,\n",
    "        \"veracity\": \"Es un sitio que no verifica noticias\",\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"author\": author,\n",
    "        \"text\": text,\n",
    "        \"links\": links,\n",
    "        \"images\": images\n",
    "    }\n",
    "    return page_data\n",
    "\n",
    "def biobiochile_special_new_process(page):\n",
    "    link = page[\"link\"]\n",
    "    soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "    date = soup.find(\"div\", class_=\"fecha\")\n",
    "    if date is None:\n",
    "        date = \"not found\"\n",
    "    else:\n",
    "        date = date.text.split(\" \")\n",
    "        month = date[2]\n",
    "        if month in months:\n",
    "            month = months.index(month) + 1\n",
    "            date = f\"{date[1]}/{month}/{date[4]}\"\n",
    "    title = soup.find(\"h1\", class_=\"titulo\").text\n",
    "    author = soup.find(\"div\", class_=\"nota\").find(\"p\", class_=\"autor\").text\n",
    "    text = \"\".join([p.text for p in soup.find(\"div\", class_=\"nota\").find(\"div\", class_=\"container-redes-contenido\").find_all(\"p\")])\n",
    "    links = list(set(a[\"href\"] for a in soup.find(\"div\", class_=\"nota\").find(\"div\", class_=\"container-redes-contenido\").find_all(\"a\")))\n",
    "    images = [{\"image\": soup.find(\"div\", class_=\"imagen-container\").find(\"img\")[\"src\"], \"text\": \"\", \"error\": \"\"}]\n",
    "    page_data = {\n",
    "        \"link\": link,\n",
    "        \"veracity\": \"Es un sitio que no verifica noticias\",\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"author\": author,\n",
    "        \"text\": text,\n",
    "        \"links\": links,\n",
    "        \"images\": images\n",
    "    }\n",
    "    return page_data\n",
    "\n",
    "biobiochile_data_extract()\n",
    "\n",
    "biobiochile_data = {}\n",
    "\n",
    "with open(f\"./archive/biobiochile_data/biobiochile_data_extracted.json\", 'r', encoding='utf-8') as file:\n",
    "        biobiochile_data = json.load(file)\n",
    "\n",
    "print(f'Se han analizado correctamente {len([page for page in biobiochile_data[\"pages\"] if \"link\" in page.keys()])} páginas de biobiochile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pages from Fact Checking UC\n",
    "\n",
    "# Posiblemente es necesario eliminar este código\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def get_pages_fact_checking():\n",
    "\n",
    "#     page_number = 1\n",
    "#     fact_checking_links = {\"facts\": []}\n",
    "    \n",
    "#     print(\"Obteniendo links\")\n",
    "#     while(True):\n",
    "#         fact_checking_link = f\"https://factchecking.cl/page/{page_number}/?s=Constitucion\"\n",
    "#         soup = BeautifulSoup(requests.get(fact_checking_link).content, 'html.parser')\n",
    "#         if soup.find('div', class_='gp-entry-content') != None:\n",
    "#             print(\"termino de busqueda\")\n",
    "#             print(len(fact_checking_links[\"facts\"]))\n",
    "#             break\n",
    "\n",
    "#         facts = [a for a in soup.find('div', class_='gp-inner-loop ajax-loop').find_all(\"a\")]\n",
    "#         for fact in facts:\n",
    "#             if fact[\"href\"].split(\"/\")[3] != \"user-review\":\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 link_data = {\n",
    "#                     \"link\": fact[\"href\"],\n",
    "#                     \"veracity\": fact.find(\"span\", class_=\"label\").find(\"span\").text\n",
    "#                 }\n",
    "#                 print(link_data)\n",
    "#                 fact_checking_links[\"facts\"].append(link_data)\n",
    "        \n",
    "         \n",
    "#         page_number += 1\n",
    "\n",
    "#     with open(f\"fact_checking_data/fact_checking_links.json\", 'w', encoding='utf-8') as file:\n",
    "#         json.dump(fact_checking_links, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# #Fact Checking\n",
    "# get_pages_fact_checking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extract | Fact Checking UC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_checking_data_extract(): \n",
    "    fact_checking_links = {}\n",
    "    fact_checking_data_extracted = {\"facts\" : []}\n",
    "\n",
    "    with open(f\"fact_checking_data/fact_checking_links.json\", 'r', encoding='utf-8') as file:\n",
    "        fact_checking_links = json.load(file)\n",
    "\n",
    "    for fact in fact_checking_links[\"facts\"]:\n",
    "        soup = BeautifulSoup(requests.get(fact[\"link\"]).content, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            year = int(soup.find(\"h2\", class_=\"gp-entry-title\").text.split(\" \")[-1])\n",
    "        except:\n",
    "            year = \"no year found\"\n",
    "        \n",
    "        link = fact[\"link\"]\n",
    "        veracity = fact[\"veracity\"]\n",
    "        title = soup.find('h1')\n",
    "        author = soup.find('h6', class_=\"gp-share-icons\")\n",
    "        content = soup.find(\"div\", class_=\"gp-entry-text\")\n",
    "        links = content.find_all(\"a\")\n",
    "        images = content.find_all(\"img\")\n",
    "\n",
    "        fact_data = {\n",
    "            \"link\": link,\n",
    "            \"veracity\": veracity,\n",
    "            \"title\": title.text,\n",
    "            \"year\": year,\n",
    "            \"author\": author.text,\n",
    "            \"content\": content.text,\n",
    "            \"links\": [link[\"href\"] for link in links],\n",
    "            \"images\": [image[\"src\"] for image in images]\n",
    "        }\n",
    "\n",
    "        fact_checking_data_extracted[\"facts\"].append(fact_data)\n",
    "\n",
    "    with open(f\"fact_checking_data/fact_checking_data_extracted.json\", 'w', encoding='utf-8') as file:\n",
    "        json.dump(fact_checking_data_extracted, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "fact_checking_data_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extract | Fast Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pages_with_errors = {\n",
    "    \"errors\": []\n",
    "}\n",
    "\n",
    "fast_check_1st = {}\n",
    "fast_check_2nd = {}\n",
    "\n",
    "with open('fast_check_data/1st_fast_check_pages.json', 'r', encoding='utf-8') as file:\n",
    "    fast_check_1st = json.load(file)\n",
    "\n",
    "with open('fast_check_data/2nd_fast_check_pages.json', 'r', encoding='utf-8') as file:\n",
    "    fast_check_2nd= json.load(file)\n",
    "\n",
    "def extract_data(response, page, index): \n",
    "    response.encoding = 'ISO-8859-1'\n",
    "    content = response.content.decode('ISO-8859-1')\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    date = soup.find('time')\n",
    "    if date is not None:\n",
    "        page[\"date\"] = date.text\n",
    "\n",
    "    author = \"\"\n",
    "    veracity = \"\"\n",
    "    text = []\n",
    "    links = []\n",
    "    images = []\n",
    "\n",
    "    extracting = \"\"\n",
    "    try: \n",
    "        if (page[\"newscast\"] == \"Fast Check CL\"):\n",
    "            title = soup.find(\"h1\", class_=\"single-post-title entry-title\").text\n",
    "            if \"#\" in title:\n",
    "                extracting = \"veracity\"\n",
    "                veracity = title[title.find(\"#\") + 1:]\n",
    "                extracting = \"author\"\n",
    "                author = soup.find(\"ul\", class_=\"meta ospm-modern clr\").find(\"a\").text\n",
    "                extracting = \"text\"\n",
    "                text = soup.find(\"div\", class_=\"entry-content clr\").text\n",
    "                links = [a[\"href\"] for a in soup.find(\"div\", class_=\"container clr\").find_all(\"a\", href=True)]\n",
    "                images = soup.find(\"div\", class_=\"entry-content clr\").find_all(\"img\", class_=\"size-full wp-image-\")\n",
    "            else:\n",
    "                print(\"     No se encontró la veracidad\")\n",
    "                veracity = \"No se encontró la veracidad\"\n",
    "                author = \"No se encontró la veracidad\"\n",
    "                text = \"No se encontró la veracidad\"\n",
    "                links = \"No se encontró la veracidad\"\n",
    "                images = \"No se encontró la veracidad\"\n",
    " \n",
    "    except Exception as e:\n",
    "        error = {\n",
    "            \"page\": page[\"link\"],\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"error_message\": str(e),\n",
    "            \"extracting\": extracting,\n",
    "            \"index\": index\n",
    "        }\n",
    "        pages_with_errors[\"errors\"].append(error)\n",
    "        print(f'   [{page[\"newscast\"]}]---> Error: {e}')\n",
    "\n",
    "    print(\"     Revisión sin errores\")\n",
    "    page[\"author\"] = author\n",
    "    page[\"veracity\"] = veracity\n",
    "    page[\"text\"] = text\n",
    "    page[\"links\"] = links\n",
    "    page[\"images\"] = images\n",
    "\n",
    "def initial_request(): \n",
    "\n",
    "    global fast_check_1st\n",
    "    global fast_check_2nd\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    status_200_count = 0\n",
    "    status_403_count = 0\n",
    "    status_404_count = 0\n",
    "    status_500_count = 0\n",
    "\n",
    "    count = 0\n",
    "    for index, page in enumerate(fast_check_1st[\"pages\"]):\n",
    "        url = page[\"link\"]\n",
    "\n",
    "        try: \n",
    "            response = requests.get(url, headers=headers)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Error: {e} | {url}')\n",
    "        \n",
    "        if response.status_code == 404:\n",
    "            status_404_count += 1\n",
    "        if response.status_code == 403:\n",
    "            status_403_count += 1\n",
    "        if response.status_code == 500:\n",
    "            status_500_count += 1\n",
    "        if response.status_code == 200:\n",
    "            status_200_count += 1\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f'{Fore.RED} {response.status_code } {Style.RESET_ALL} | {url}')\n",
    "            fast_check_1st[\"pages\"][index][\"error\"] = \"True\"\n",
    "        else:\n",
    "            print(f'{Fore.GREEN} {response.status_code } {Style.RESET_ALL} | {url}')\n",
    "            extract_data(response, page, index)\n",
    "\n",
    "    for index, page in enumerate(fast_check_2nd[\"pages\"]):\n",
    "        url = page[\"link\"]\n",
    "\n",
    "        try: \n",
    "            response = requests.get(url, headers=headers)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Error: {e} | {url}')\n",
    "        \n",
    "        if response.status_code == 404:\n",
    "            status_404_count += 1\n",
    "        if response.status_code == 403:\n",
    "            status_403_count += 1\n",
    "        if response.status_code == 500:\n",
    "            status_500_count += 1\n",
    "        if response.status_code == 200:\n",
    "            status_200_count += 1\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f'{Fore.RED} {response.status_code } {Style.RESET_ALL} | {url}')\n",
    "            fast_check_2nd[\"pages\"][index][\"error\"] = \"True\"\n",
    "        else:\n",
    "            print(f'{Fore.GREEN} {response.status_code } {Style.RESET_ALL} | {url}')\n",
    "            extract_data(response, page, index)\n",
    "\n",
    "    print(\"Resume: \")\n",
    "    print(f\"Status 200: {status_200_count}\")\n",
    "    print(f\"Status 403: {status_403_count}\")\n",
    "    print(f\"Status 404: {status_404_count}\")\n",
    "    print(f\"Status 500: {status_500_count}\")\n",
    "\n",
    "initial_request()\n",
    "\n",
    "with open(\"fast_check_data/1st_fast_check_pages.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(fast_check_1st, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"fast_check_data/2nd_fast_check_pages.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(fast_check_2nd, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"pages_with_errors_fast_check.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(pages_with_errors, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages = 0\n",
    "fast_check_1st_true = 0\n",
    "fast_check_1st_false = 0\n",
    "fast_check_1st_other = 0\n",
    "\n",
    "fast_check_2nd_true = 0\n",
    "fast_check_2nd_false = 0\n",
    "fast_check_2nd_other = 0\n",
    "\n",
    "fact_checking_true = 0\n",
    "fact_checking_false = 0\n",
    "fact_checking_other = 0\n",
    "\n",
    "# fact_checking_creible = 0\n",
    "# fact_checking_no_creible = 0\n",
    "# fact_checking_impreciso = 0\n",
    "# fact_checking_engañoso = 0\n",
    "\n",
    "with open('fact_checking_data/fact_checking_links.json', 'r', encoding='utf-8') as file:\n",
    "    json_pages_info = json.load(file)\n",
    "    total_pages += len(json_pages_info[\"facts\"])\n",
    "    print(len(json_pages_info[\"facts\"]))\n",
    "    for fact in json_pages_info[\"facts\"]:\n",
    "        if fact[\"veracity\"] == \"Verdadero\":\n",
    "            fact_checking_true += 1\n",
    "        elif fact[\"veracity\"] == \"Falso\":\n",
    "            fact_checking_false += 1\n",
    "        else:\n",
    "            fact_checking_other += 1\n",
    "        # else:\n",
    "        #     fact_checking_false += 1\n",
    "\n",
    "        # elif fact[\"veracity\"] == \"Creíble\" or fact[\"veracity\"] == \"Creíble, pero...\" or fact[\"veracity\"] == \"Sería creíble, pero...\":\n",
    "        #     fact_checking_creible += 1\n",
    "        # elif fact[\"veracity\"] == \"No es creíble\":\n",
    "        #     fact_checking_no_creible += 1\n",
    "        # elif fact[\"veracity\"] == \"Impreciso\" or fact[\"veracity\"] == \"Se puso creativ@\":\n",
    "        #     fact_checking_impreciso += 1\n",
    "        # elif fact[\"veracity\"] == \"Engañoso\" or fact[\"veracity\"] == \"Ciencia Ficción\":\n",
    "        #     fact_checking_engañoso += 1\n",
    "        # else:\n",
    "        #     print(fact[\"veracity\"])\n",
    "\n",
    "with open('fast_check_data/1st_fast_check_pages.json', 'r', encoding='utf-8') as file:\n",
    "    json_pages_info = json.load(file)\n",
    "    total_pages += len(json_pages_info[\"pages\"])\n",
    "    print(len(json_pages_info[\"pages\"]))\n",
    "    for link in json_pages_info[\"pages\"]:\n",
    "        if link[\"veracity\"] == \"Real\":\n",
    "            fast_check_1st_true += 1\n",
    "        elif link[\"veracity\"] == \"Falso\":\n",
    "            fast_check_1st_false += 1\n",
    "        else:\n",
    "            fast_check_1st_other += 1\n",
    "            \n",
    "\n",
    "with open('fast_check_data/2nd_fast_check_pages.json', 'r', encoding='utf-8') as file:\n",
    "    json_pages_info = json.load(file)\n",
    "    total_pages += len(json_pages_info[\"pages\"])\n",
    "    print(len(json_pages_info[\"pages\"]))\n",
    "    for link in json_pages_info[\"pages\"]:\n",
    "        if link[\"veracity\"] == \"Real\":\n",
    "            fast_check_2nd_true += 1\n",
    "        elif link[\"veracity\"] == \"Falso\":\n",
    "            fast_check_2nd_false += 1\n",
    "        else:\n",
    "            fast_check_2nd_other += 1\n",
    "print(f\"Total pages {total_pages}\")\n",
    "print(\"Fast Check 1st\")\n",
    "print(f\"True: {fast_check_1st_true}\")\n",
    "print(f\"False: {fast_check_1st_false}\")\n",
    "print(f\"Other: {fast_check_1st_other}\")\n",
    "print(\"Fast Check 2nd\")\n",
    "print(f\"True: {fast_check_2nd_true}\")\n",
    "print(f\"False: {fast_check_2nd_false}\")\n",
    "print(f\"Other: {fast_check_2nd_other}\")\n",
    "print(\"Fact Checking\")\n",
    "print(f\"True: {fact_checking_true}\")\n",
    "print(f\"False: {fact_checking_false}\")\n",
    "print(f\"Other: {fact_checking_other}\")\n",
    "# print(f\"Creible: {fact_checking_creible}\")\n",
    "# print(f\"No Creible: {fact_checking_no_creible}\")\n",
    "# print(f\"Impreciso: {fact_checking_impreciso}\")\n",
    "# print(f\"Engañoso: {fact_checking_engañoso}\")\n",
    "print(f\"suma : {fact_checking_false + fact_checking_true + fact_checking_other}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
